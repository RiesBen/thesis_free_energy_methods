\section{History of molecular modeling}

%From time immemorial
Since ancient times, humans have strived to understand their environment
and the phenomena they observed.
%
They have conceived models describing the composition of the surrounding matter
and attempting to describe its behavior.
%
The starting point was
%were 
century-long disputes between representatives %supporters? followers?
of atomic and continuum theories. The atomic theory was first proposed\cite{MA83.6} in the 5$^{\mathrm{th}}$ century BC
% https://plato.stanford.edu/entries/democritus/#2
by Presocratics Leucippus and Democritus. This theory argued that matter is not
infinitely divisible, but that there ultimately
%are least 
particles which are
inalterable and indivisible: the atoms.
%
It was Democritus, who claimed\cite{DI51.1,TA99.1}
%H. Diels and W. Kranz, Die Fragmente der Vorsokratiker, 6th edition, Berlin: Weidmann, 1951.,C.C.W. Taylor, The Atomists: Leucippus and Democritus. Fragments, A Text and Translation with Commentary, Toronto: University of Toronto Press, 1999a. }%  (DK 68B9, trans. Taylor 1999a).
%
\begin{displayquote}
``by convention sweet and by convention bitter, by convention hot, by convention cold, by convention color; but in reality atoms and void''.
\end{displayquote}
%
This groundbreaking statement was far ahead of its time and was mostly rejected, especially
by Aristotle with his work \textit{De caelo}, stating that matter is continuous and consists
%out 
of five elements.\cite{ST30.1} As the church 
%was 
adhered to the continuity theory of Aristotle, 
it was predominant in Europe, until the 
%greek atomism 
theory of Democritus
experienced a revival 
with 
% rise of 
the mechanical atomism in the 17$^{\mathrm{th}}$ century, promoted by philosophers 
like Ren\'e Descartes, Pierre Gassendi and Robert Boyle.\cite{CH09.22}

While Newton led the foundations of classical mechanics in his work \textit{Philosophiae Naturalis Principia Mathematica},\cite{NE99.1} he 
also shared atomistic views as he stated that\cite{CA62.1}
%
\begin{displayquote}
``the least parts of bodies to be - all extended, and hard and impenetrable, and moveable, and endowed with their proper inertia''.% (Cajori, 1962, 399)
\end{displayquote}
%
Before, the atomists were uncertain about the laws governing the
movements of atoms. With Newton's three laws of motion, the dynamics
of atoms could in principle be determined. The limitation, however, 
was that the nature of 
%all the 
forces between atoms needed to be known.\cite{CH14.20}%% @InCollection{sep-atomism-modern,
%% 	author       =	{Chalmers, Alan},
%% 	title        =	{Atomism from the 17th to the 20th Century},
%% 	booktitle    =	{The Stanford Encyclopedia of Philosophy},
%% 	editor       =	{Edward N. Zalta},
%% 	howpublished =	{\url{https://plato.stanford.edu/archives/win2014/entries/atomism-modern/}},
%% 	year         =	{2014},
%% 	edition      =	{Winter 2014},
%% 	publisher    =	{Metaphysics Research Lab, Stanford University}
%% }

The early mechanical atomism was then superseded by the atomic theory
of John Dalton in the early 19$^{\mathrm{th}}$ century.\cite{CH14.20} For the first
time, properties, such as the relative weight \phicom{density?} could be assigned to
atoms. Chemical elements were already known, but now
atoms could be ascribed to 
%least units 
smallest unit of what was understood as an element. Likewise, chemical 
compounds were found to be defined by specific combinations of atoms.

The word ``chemical structure'' was 
%termed 
coined in the mid 19$^{\mathrm{th}}$ century by
chemists like Archibald Scott Couper, Friedrich August Kekul\'e and
Alexander Mikhailovich Butlerov. They 
%have 
proposed the first
chemical structures, thereby developing important 
concepts like valency, chemical bond and substituent.\cite{SC98.6}
In 1861, Johann Josef Loschmidt published a collection of 384 molecular structures,
some examples of which are given in \reffigs{loschmidt} and \reffign{tol}. His graphical
representations
 included the spatial extent of different atoms in surprisingly correct proportions
%considering the knowledge at the time.
considering the publication date.
Additionally, he proposed the first (correct!) benzene structure, although
credit is often incorrectly given to Kekul\'e. However, the latter invented the
resonance formulas of benzene, which are showcased in \reffig{kekule}.\cite{KE57.1,KE66.1,KE72.1,RO85.3}
%publication date.

The third spatial dimension came into play with August Wilhelm Hofmann, who created, using table crocket balls, three-dimensional
models of methane (\reffig{hofmann}), chloroform, and other small organic molecules.\cite{HO04.13}
%% BOOK
%% TI  - Models: The Third Dimension of Science
%% A2  - Hopwood, Nick
%% A2  - de Chadarevian, Soraya
%% SP  - 488
%% CY  - Stanford
%% PB  - Stanford University Press
%% PY  - 2004
%% UR  - http://www.sup.org/books/title/?id=1242
%% %% Y2  - 2019/03/03
%% author = {Meinel, Christoph},
%% year = {2004},
%% month = {01},
%% pages = {242-275},
%% title = {Molecules and croquet balls},
%% journal = {Models: The Third Dimension of Science}
%% }
Although the structures were not all in agreement with today's state of knowledge,
he 
%still 
was the one who devised the color scheme for atoms (\eg{} black for carbon, white for hydrogen)
which is still in use today. 
%D. Crowfoot, C. W. Bunn, B. W. Rogers-Low, A. Turner-Jones, X-ray Crystallographic Investigation of the Structure of Penicillin (Princeton Univ. Press, 1949), pp. 310–367
In the following years such models were progressively refined, following both experimental
and theoretical advances. Structural representations
of larger and more complex (bio)molecules were created.
%Notable 
Important milestones in this respect were the model of penicillin based on X-ray 
crystallographic data by Hodgkin et al.\cite{CR49.1},
%
the model of protein $\alpha$-helices by Pauling et al.\cite{PA51.2},%L. Pauling, R. B. Corey, H. R. Branson, The structure of proteins: Two hydrogen-bonded helical configurations of the polypeptide chain. Proc. Natl. Acad. Sci. U.S.A. 37, 205–211 (1951)
and the famous DNA model by Crick and Watson.\cite{CR54.1}%F. H. C. Crick, J. D. Watson, The complementary structure of deoxyribonucleic acid. Proc. R. Soc. A 223, 80–96 (1954).

\newcommand{\subgraphpngint}[3]{
  \begin{minipage}[t]{2em}\subcaption{}\label{#1}\end{minipage}%
  \begin{minipage}[t]{\dimexpr#3\linewidth\relax}
  \centering
%  \vspace{-.5\baselineskip}
  \hfill \\
  \includegraphics[width=\textwidth]{\path/fig/#2}
  \end{minipage}%
}

\begin{figure}[!htb]
\centering
\begin{minipage}[t]{.5\textwidth}
\subgraphpngint{fig:loschmidt}{loschmidt.png}{0.9}%
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}
\subgraphpngint{fig:tol}{tol.png}{0.3}\\[.2cm]
\subgraphpngint{fig:kekule}{kekule.png}{0.9}%
\end{minipage}\\[.2cm]
\subgraphpngint{fig:hofmann}{hofmann.jpg}{0.245}%
\hfill
\subgraphpngint{fig:kendrew}{kendrew.jpg}{0.5}
\caption{\footnotesize\captitital{Examples of early molecular models.}
Graphs (\subref{fig:loschmidt}) and (\subref{fig:tol}) show schemes copied from ``Konstitutionsformeln der organischen Chemie in graphischer Darstellung'' by J. Loschmidt from 1961, pp. 7 and 62 of Ref.\cite{LO61.1}
 (\subref{fig:loschmidt}): Illustrations of the structures of methane, carbon dioxide, formaldehyde, methanol, formic acid and carbonic acid.
 (\subref{fig:tol}): Structure of toluene with the first correct prediction of the benzene ring copied from Ref.\cite{KE72.1}.
 (\subref{fig:kekule}): Drawings of the two resonance formulas of benzene by Kekul\'e. Copied from 
 (\subref{fig:hofmann}): Photograph of a methane molecular model, created out of table crocket balls around 1860 by August Wilhelm von Hofmann, which is now part of the collection of the Royal Institution of London. (It is photographed by Henry Rzepa, with the kind permission of the Royal Institution of London, in whose collection the model resides. Wikimedia Commons, the free media repository. Accessed March 10, 2019. \texttt{https://commons.wikimedia.org/wiki/File:Molecular\_Model\_of\_Methane\_\-Hofmann.jpg}, CC BY-SA 4.0)
 (\subref{fig:kendrew}) shows the first model of a complete protein, created and published in 1958 by Kendrew et al.\cite{KE58.1} (Science Museum Group. Kendrew's original model of the myoglobin molecule. 1975-533. Science Museum Group Collection Online. Accessed March 10, 2019. \texttt{https://collection.sciencemuseum.org.uk/objects/co13543.1.1}, CC BY-NC-SA)% This image is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Licence
}
\end{figure}


Finally, in 1958, the first (though still coarse) model of a complete protein was proposed
%resolved 
by Kendrew et al.\cite{KE58.1} (based on the work by Perutz). %M. F. Perutz, M. G. Rossmann, A. F. Cullis, H. Muirhead, G. Will, A. C. T. North, Structure of haemoglobin: A three-dimensional Fourier synthesis at 5.5-Å. Resolution, obtained by x-ray analysis. Nature 185, 416–422 (1960).
 It was a model of myoglobin made out of wood, plasticine and paint, and is shown in \reffig{kendrew}. A model of the structurally similar haemoglobin was created by Perutz et al. a few years later.\cite{PE60.1}
%M. F. Perutz, M. G. Rossmann, A. F. Cullis, H. Muirhead, G. Will, A. C. T. North, Structure of haemoglobin: A three-dimensional Fourier synthesis at 5.5-Å. Resolution, obtained by x-ray analysis. Nature 185, 416–422 (1960).


%% Molecular model of methane, created by August Wilhelm von Hofmann (ca. 1860). The square planar structure depicted is now known to be incorrect.
%% Nederlands: Molecuulmodel van methaan door August Wilhelm von Hofmann (ca. 1860).
%% Date	7 July 2005
%% Source	Self-photographed
%% (Original caption: “Photographed by Henry Rzepa, with the kind permission of the Royal Institution of London, in whose collection the model resides”)
%% Author	Henry Rzepa

% Since the late forties, computational physicists have developed new numerical techniques (Monte Carlo and molecular dynamics simulation, fast Fourier transformation), discovered unexpected physical phenomena (Alder vortices, shear thinning), and posed new questions to theory and experiment (chaos, strange attractors, cellular automata, neural nets, spin glasses, ...).

The advent of digital computers in the 1940s opened up unprecedented
%
%entirely new 
possibilities for
building molecular models. Computers were not only used to resolve the X-ray structures of
the proteins mentioned above, but also to solve the equations of the physical models
which were otherwise intractable. 
%
%In addition to quantum-chemical calculations, these were classical mechanics simulations.
These included both quantum-chemical as well as classical-mechanics simulations.
%
After the first Monte Carlo sampling\cite{ME53.1}
and then molecular dynamics simulation\cite{AL57.1}
of hard disks in 2 dimensions in the 1950s, 
the first Lennard-Jones fluid was simulated in 1964.\cite{RA64.1}
Computer simulations added a new and essential 
aspect to the molecular models: the alteration of the structure with time, resulting in 
translations, vibrations, and conformational changes of molecules.
%
\phicom{killed: ``This allow reactions and are related to properties of chemical compounds.''
as it was unclear}
%
The crucial roal
of the atomic and molecular motions 
in defining material properties and - ultimately - life 
was famously phrased by Feynman who stated\cite{FE63.1}
%
\begin{displayquote}
``that all things are made of atoms, and that everything that living things do can be understood in terms of the jigglings and wigglings of atoms.''
\end{displayquote}
%''Certainly no subject or field is making more progress on so many fronts at the present moment, than biology, and if we were to name the most powerful assumption of all, which leads one on and on in an attempt to understand life, it is that all things are made of atoms, and that everything that living things do can be understood in terms of the jigglings and wigglings of atoms.''
%
%Thus, 
The new field of molecular simulation was born, contributing to the
 understanding of matter and life in all its details at the atomistic
level.
%


Polyatomic molecules like water\cite{ST74.1} and proteins\cite{MC77.1} were simulated 
shortly thereafter. In the following decades, progress was made in
developing force fields (interaction functions) and algortihms
for reliable, efficient and accurate molecular dynamics (MD) simulations.
Since the 1990s, MD has become 
%it became 
an indispensable tool in 
%many 
nearly all fields
of science,
including
%like 
physics, chemistry, biology, pharmaceutical and material science, and medicine.
%
Due to the ever increasing computational power, larger systems become tractable, 
which in turn increases the scope of the applications.
Apart from visualizing molecules for a better understanding, 
computations
%simulation
% also 
provide a mean for calculating properties, thus complementing
and even, sometimes, substituting experiments.


%With virtual reality molecules can be experienced and increase the understanding and 
%perception of models.

\section{Classical Mechanics}
\labsec{cmech}

The foundations of classical mechanics were laid by Newton,\cite{NE99.1} 
and later 
%reformulated
generalized 
by Lagrange and Hamilton.\phicom{[REF]}
While the laws of classical mechanics are not valid when
dealing with high energies (\eg{} high velocities, light-matter interactions)
or low masses (\eg{} electrons, other elementary particles), where quantum or/and relativistic
%mechanics 
%and relativistic effects 
mechanics is required, they can provide 
%the required accuracy for more
a sufficient accuracy for understanding
macroscopic phenomena at a molecular level with 
comparatively low computational costs.



Classical systems are usually described by $N$ point particles $i=0,1,\dots,N-1$
with mass $m_i$ in Cartesian space, \ie{} using $x$, $y$ and $z$ coordinates.
Each particle resides at a certain position $\rv_i=\{x_i, y_i, z_i\}$ with a
momentum $\pv_i=m_i\fvector{v}_i=m_i\{v_{x,i},v_{y,i},v_{z,i}\}$ where $\fvector{v}$
is the velocity. The vectors $\rv$ and $\pv$ can be united in a position-momentum vector $\xv_i=\{\rv_i,\pv_i\}$.
The vectors $\xv_i$ of all particles are the elements of the phase-space vector $\xv=\{\xv_0, \xv_1,\dots,\xv_{N-1}\}$ which contains the information about the state of a system. 
\phicom{made the $\xv$ lower case - because later you use $\Xv$ for a replica system}
The phase space
is the associated $6N$-dimensional space, which 
%includes all possible values of $\xv$, \ie{} it
%contains 
is defined by all possible combinations of 
%different 
positions and momenta of all
particles $i$.

The associated energy is given by the so-called Hamilonian $\ham$, which is a function
of the phase space vector $\xv$, and can be separated into a kinetic-energy term $\mathcal{K}$
and a potential-energy term $\mathcal{V}$ of the system,
%
\begin{align}
 \mathcal{H}(\xv) &= \mathcal{K}(\fvector{p}) + \mathcal{V}(\fvector{r}). \label{eq:ham}
\end{align}
%
The kinetic energy is given by
%
\begin{align}
   \mathcal{K}(\fvector{p}) = \sum_i \frac{\fvector{p}_i^2}{2 m_i}, \label{eq:kinetic}
\end{align}
%
which is the sum over the kinetic energies of all particles.
%
The potential energy term is a mathematical expression for the
interaction between the particles, and possibly with the environment.
It can 
be given
%exhibit 
different functional forms, depending on the nature of the
system and the level of approximation,
which is typically chosen according to the desired level of accuracy.


The time evolution of the system in Hamiltonian mechanics is given by two equations.
The first equation describes how the positions evolve in time,
%% %% %
\begin{align}
 \dot{\fvector{r}}_i &= \frac{\dd \fvector{r}_i}{\dd t} =   \frac{\partial \mathcal{H}}{\partial \fvector{p}_i}. \label{eq:ri}
 \end{align}
 %
 By inserting the definition of the kinetic energy, \refeq{kinetic}, one obtains
 $ \dot{\fvector{r}}_i = \partial \mathcal{H} / \partial \fvector{p}_i
    = \partial \mathcal{K} / \partial \fvector{p}_i
    = \fvector{p}_i / m_i = \fvector{v}_i$,
 which tells us that the positions change in time according
 to the velocities of the particles. The second equation describes the
 time evolution of the momenta, 
%
\begin{align}  
 \dot{\fvector{p}}_i &= \frac{\dd \fvector{p}_i}{\dd t} = - \frac{\partial \mathcal{H}}{\partial \fvector{r}_i} \label{eq:pi} . 
\end{align}
%
The time derivative of the momentum of a particle $i$ is equal to its acceleration $a_i$
%scaled 
multiplied by its mass, $\dot{\pv}_i=m_i\dot{\fvector{v}}_i=m_ia_i$.
Therefore the second equation is another formulation of
Newton's 
%famous 
second law $\fvector{f}_i = m_i \fvector{a}_i$,
where $\fvector{f}_i$ is the force, given by the negative
gradient of the Hamiltonian function with respect to the
position $\fvector{f}_i=-\partial \mathcal{H}/\partial \fvector{r}_i$.

It is important to note that the total energy of a system governed
by \refeqs{ri} and \refeqn{pi} is constant, \ie{} $\mathcal{H}$ in
Hamiltonian mechanics is a constant of motion.
%
\phicom{JUST FOR YOU: 
This is not entirely true in generalized coordinate systems, though;
it requires also that the kinetic energy is a quadratic function of 
the generalized momenta (obviously true in a Cartesian coordinate system,
of course) - so, your formulation is correct}

\section{Classical Statistical Mechanics}

Statistical mechanics\cite{FR85.2,KU90.4,MC00.1,NA06.1,HU10.10} is a branch of physics
which links 
the macrosopic properties to the 
microscopic behavior of a system.
%
The microscopic behavior of the system
is governed by the interactions of its particles, \ie{} by the interaction function $\mathcal{V}$ in \refeq{ham}.
In classical statistical mechanics, the interaction functions are formulated in the framework of
classical mechanics and are part of the Hamiltonian of the system. Because of the immense size
of macroscopic systems ($1\unit{mol}=6.022\cdot 10^{23}$ particles) and the non-trivial microscopic
interactions between particles in real systems, the treatment even according to classical
mechanics 
%to calculate the properties of interest 
is generally not feasible analytically, 
except for the simplest systems (ideal gas, harmonic crystal).
\phicom{check last sentence, a bit reformulated}
%
However, by statistically evaluating the microscopic details, 
we can still calculate the macroscopic properties of a system.
\phicom{last sentence unclear - do you mean we do it *numerically* instead;
or did you mean something totally different? - to be refined}
%

%
The statistical evaluation is performed on the basis of an ensemble, which is a collection of microscopic states (microstates) of the same system obeying the same microscopic interactions and fulfilling certain macroscopic boundary conditions. A macroscopic property of interest can then be calculated by averaging over the ensemble.
%
\phicom{I would remove the italics - you never use them before}
%
According to the ergodic hypothesis,\cite{GA16.8} this property can also be calculated by observing the evolution
of one system over an infinite period of time. \phicom{changed to infinite}
%
% Ergodic hypthesis ?
%
A microstate is defined by a point $\xv=\{\rv,\pv\}$ in the phase space of the system.
%, which describes
%all the coordinates and momenta of the particles in the system. 
Typically, a system
treated by statistical mechanics has many degrees of freedom, \ie{} a high-dimensional $\xv$.
%
An ensemble encompasses all microstates which obey some given macroscopic boundary conditions,
which are usually three constant thermodynamic observables, among them at least
one extensive quantity.
%quantities describing the macroscopic state of the system.
Common ensembles are:
($i$) the
microcanonical ensemble with constant number $N$ of particles, volume $V$ and energy $E$;
($ii$)
the canonical ensemble with constant number $N$ of particles, volume $V$ and temperature $T$;
($iii$) the isothermal-isobaric (Gibbs) ensemble with constant number $N$ of particles, pressure $P$ and temperature $T$;
($iv$)
the grand canoncial ensemble with constant chemical potential $\mu$, volume $V$ and temperature $T$.
%
In the following, we consider only the microcanonical and canonical ensembles, but the discussion can be straightforwardly extended to other systems.
%

\subsection{The microcanonical ensemble}
%
The microcanonical ensemble is the simplest ensemble, corresponding to an isolated system
with a constant number $N$ of particles in a container of constant volume $V$ at constant energy $E$.

To be part of the ensemble, the microstates defined by the phase space vector $\xv$ have to fulfill the condition 
%
\begin{align}
\mathcal{H}(\xv) &= E.
\end{align}
%
All accessible microstates $\xv$ of the ensemble lie on a constant-energy hypersurface in phase space
and are equally probable. This is known as the assumption of equal a priori probability.
%

%
A measure of the amount of phase space available to the system is the partition function. The microcanonical partition function $\Omega$ is calculated as an integral over phase space
%
\begin{align}
        \Omega(N,V,E) %&= \frac{E_0}{N!h^{3N}} \int \dd \pv \dd \rv \delta(\ham(\rv,\pv) - E ) \\
                      &= \frac{E_0}{N!h^{3N}} \int \dd \xv \delta(\ham(\xv) - E ),
\end{align}
%
where the prefactor 
%$E_0N!^{-1}h^{-3N}$ 
accounts for particle indistinguishability,
%($N!^{-1}$)
unit conversion to a dimensionless quantity,
% ($E_0$) 
and Heisenberg's uncertainty relation.
% ($h^{-3N}$).
%
The partition function is a fundamental quantity in statistical mechanics. All thermodynamic properties of the system can be derived from it. %
A property related to the number of microstates and, therefore, the partition function is the entropy $S$, which is linked to $\Omega$ via the Boltzmann relation
%
\begin{align}
        S(N,V,E) &= \kb \ln \Omega (N,V,E)\ ,
        \label{eq:boltzmann}
\end{align}
%
where the proportionality factor $\kb=8.3146\unit{J\,K^{-1}mol^{-1}}$ is the Boltzmann constant. 
For a microcanonical ensemble, the entropy indicates which macrostate is favored.
Given two different macrostates, the system will always prefer the state with the higher entropy. A process to this state will always occur spontaneously. However, the entropy difference does not 
provide
%reveal 
any information on the timescale of the corresponding process, which can
%could 
be short or 
%very 
long due to kinetic hindrance. Other thermodynamic properties can be derived by taking the total differential of the entropy $S(N,V,E)$, known from phenomenological thermodynamics,
%
\begin{align}
\dd S &= \left ( \frac{\partial S}{\partial E}\right ) _{N,V} \dd E +
\left ( \frac{\partial S}{\partial V} \right ) _{N,E} \dd V + \left ( \frac{\partial S}{\partial N} \right ) _{V,E} \dd N  \nonumber \\
 & = \frac{1}{T} \dd E + \frac{P}{T} \dd V - \frac{\mu}{T} \dd N\ ,
 \label{eq:tots}
\end{align}
%
where $T$ is the temperature, $P$ the pressure and $\mu$ the chemical potential.
If one equates the coefficients, \ie{}
%
\begin{align}
\frac{1}{T}&=\left ( \frac{\partial S}{\partial E} \right ) _{N,V},\nonumber \\
\frac{P}{T}&=\left ( \frac{\partial S}{\partial V} \right ) _{N,E}, \nonumber \\
\frac{\mu}{T}&=\left ( \frac{\partial S}{\partial N} \right ) _{V,E}\ ,
\label{eq:coeffs}
\end{align}
%
one can relate the thermodynamic variables $T$, $P$, and $\mu$ to $\Omega(N,V,E)$ by combining \refeq{boltzmann} with \refeq{coeffs}:
%
\begin{align}
\beta&=\left ( \frac{\partial \ln \Omega}{\partial E} \right ) _{N,V}, \nonumber \\
\beta P&=\left ( \frac{\partial \ln \Omega}{\partial V} \right ) _{N,E} \nonumber \\
\beta \mu&=\left ( \frac{\partial \ln \Omega}{\partial N} \right ) _{V,E}.
\end{align}
%
The normalized probability $P(\xv)$ of finding a system in a given microstate $\xv$ can also be related to the inverse partition function
%
\begin{align}
    P(\xv) &= \frac{E_0}{N!h^{3N}}\frac{\delta(\ham(\xv)-E)}{\Omega(N,V,E)} . \label{eq:microprob}
\end{align}
%
The normalized probability defines the distribution of macrostates in equilibrium. Accordingly, any thermodynamic observable $Y(N,V,E)$ can be calculated by multiplying the instantaneous quantity $\mathcal{Y(\xv)}$ of a microstate with its probability and integrating over the whole phase space, \ie
%
\begin{align}
    Y(N,V,E)&=\langle \mathcal{Y}(\xv) \rangle \nonumber \\
     &=\int \dd \xv \, P(\xv) \mathcal{Y}(\xv) \label{eq:property}
\end{align}
%
where $\langle \dots \rangle$ denotes the ensemble average. Note that 
\refeq{property} is also 
valid for other ensembles, but the probability, given by \refeq{microprob} for a microcanonical
ensemble, has to be adapted.

While being the simplest ensemble, the use of the microcanocial ensemble is limited, because most experiments are conducted under isothermal conditions.
%
\phicom{I disagree. An isolated system is relatively easy to build.
Isochor+adiabatic is all it needs! - the real reason is that most experiments
in chemistry involve isothermal conditions.}


%
\subsection{The canonical ensemble}
%
The canonical ensemble is an ensemble 
of systems
with a constant
number $N$ particles in a container of constant volume $V$ at
temperature $T$, \ie{} the systems are in contact with a heat bath
allowing the exchange of energy (heat) in order to keep the temperature of
the system constant.
%

%
The main difference to the microcanonical ensemble 
%consists in
resides in
the heat exchange with the environment.
The total energy is not
conserved
and the microstates now
%, thus the total energy of the system 
obey a Boltzmann
distribution, with probabilities proportional to $\exp(-\beta \ham(\xv))$,
where $\beta=(k_{\mathrm{B}}T)^{-1}$ and $k_{\mathrm{B}}$ is the Boltzmann's constant,.
\phicom{define beta}  The canonical partition
function is an integral over the Boltzmann factor for all possible
microstates
%
\begin{align}
    Q(N, V, T) &= \frac{1}{h^{3N}N!}\int \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}(\xv)) .
\label{eq:partfct}
\end{align}
%
The energy of the ensemble $E$
is the ensemble average, 
given by \refeq{property}, of the Hamiltonian.
%
\begin{align}
    E(N,V,T)&=\langle \mathcal{H}(\xv) \rangle. \nonumber\\
    &=\int \mathrm{d}\xv\, P(\xv) \mathcal{H}(\xv) \nonumber \\
    &=-\frac{1}{Q(N,V,T)} \frac{\partial}{\partial \beta}Q(N,V,T) \nonumber \\
    &=-\frac{\partial}{\partial \beta} \ln Q(N,V,T) .
\end{align}
%
\phicom{lots of equation numbers in this single equation}
%% %
%% \begin{align}
%%     S(N,V,T)&=C - \kb \int \mathrm{d}\pv \int \mathrm{d} \rv \, P(\rv, \pv) \ln P(\rv, \pv)
%% \end{align}

For a canonical ensemble, the entropy difference between two states does not
tell anything about the spontaneity of a process, because the 
%respective
corresponding entropy  change in the environment (resulting from the transfer of heat)
has to be taken into account.
Therefore it is convenient to define the Helmholtz free energy $A$ as
%
\begin{align}
    A(N,V,T)&=-\beta^{-1} \ln Q(N,V,T).
\end{align}
%
This quantity 
%does 
gives information about the spontaneity of a process. If the
free-energy difference is negative, the process will occur. If it is zero, 
the two states are in equilibrium. 
%


%===========================================================

\section{Molecular Dynamics Simulation}

Molecular dynamics (MD) simulations apply classical mechanics (\refsec{cmech})
to molecular systems. Common applications include organic or biological molecules in solvent.
In the following, the framework of MD is briefly introduced.

\subsection{The System}

A typical simulated system consists of one or more solute(s), 
\eg{} an organic molecule, a protein, a lipid, a saccharide,
a nucleic acid or a combination of those.
The environment of this
solute is either vacuum, implicit solvent (which models the average
influence of solvent molecules \via{} a continuous medium)
or explicit solvent (solvent molecules are  added to the system).
The number of atoms can be on the order of several 
%$1\,000\,000$ 
millions of
atoms, which leads to system sizes on the order of $10\times10\times10\unit{nm}^3$. The
times simulated are on the order of nanoseconds to
milliseconds per day computer time, depending on the
available computing resources, on the system size,
on the employed approximations and on the algorithms used.

The 
%corresponding 
atoms to be simulated are described as
$N$ point particles having masses $\{m_i\}$ and charges $\{q_i\}$. Often,
a number of atoms are combined to one point particle, which is
referred to as an united atom\cite{DA98.3} (\eg{} a methyl group) or a
coarse grained bead\cite{MA07.4,VO08.1} (\eg{}, a whole ester functional group).
The electrons and other elementary particles are not included explicitly.
%are neglected. 
Only their average influence is modeled by means
of the masses and charges. Therefore, MD simulations cannot describe
electronically excited states, 
%which includes 
chemical reactions or
light-matter interactions.
%
In the case of explicit solvation, periodic boundary conditions are
usually applied to avoid surface effects. The system is then modeled in a space-filling
polyhedron, \eg{}, a cube or a truncated octahedron, and an atom leaving the boundary
on one side enters again on the opposite side. 
In other words, 
%Additionally, the 
the system is allowed to interact
with periodic copies of itself.
%, representing a continuous system.


\subsection{The Interaction Function (Force Field)}
\label{sec:force_field}

The potential energy $\mathcal{V}$ (\refsec{cmech})
is a mathematical expression to describe the nature of the interactions within the system.
In MD, this function is referred to as a force field.
% which defines
%the interaction between the point particles in the system. 
A force field consists of many terms, which can be classified as
covalent and non-bonded terms.\cite{OO04.1} Usually non-bonded terms are two-body
terms, \ie{} they include only the coordinates of two point particles.
Some $n$-body terms with $n>2$ can be considered in exceptional cases.
\phicom{this is not true - or at least not precise - besides bonds, covalent terms
are not 2-body terms - they are cheap because there are only
few of them}

The covalent terms describe the interactions within one
molecule up to third-neighbor interactions, \ie{} between
atoms separated by up to three chemical bonds. They usually include:
%
($i$) bond stretching terms, which are harmonic potentials controlling the distance
between two connected atoms;
($ii$) bond-angle bending terms, which are harmonic potentials controlling the angle
spanned by three atoms;
($iii$) torsional-dihedral terms, which are usually cosine series controlling
the configuration of four consecutively bonded atoms;
($iv$) improper-dihedral terms, which are harmonic potentials controlling
the configuration of four atoms, of which three are bonded to a central atom.
Other terms ({\em e.g.} covalent cross-terms) are possible, but seldomly used.

%% %% The first group includes the terms $U_\mathrm{bond}$, $U_\mathrm{angle}$, 
%% %% $U_\mathrm{dih}$ and $U_\mathrm{imp}$ for controlling the bond 
%% %% lengths, angles, dihedral angles and improper dihedral angles, respectively
%% %% %
%% %% \begin{align}
%% %%  U_\mathrm{bond}  &= \sum\limits_{i,j} \frac{1}{4} k_{i,j} ( r_{i,j}^2 
%% %%                     - (r_{i,j}^\mathrm{ref})^2)^2 \label{eq:bond_term} \\
%% %%  U_\mathrm{angle} &= \sum\limits_{i,j,k} \frac{1}{2} k_{i,j,k} ( \cos \theta_{i,j,k} 
%% %%                         - \cos \theta^\mathrm{ref}_{i,j,k})^2 \\
%% %%  U_\mathrm{dih}   &= \sum\limits_{i,j,k,l} k_{i,j,k,l} \left[ 1 + \cos(m_{i,j,k,l} 
%% %%                     \phi_{i,j,k,l} - \phi^\mathrm{ref}_{i,j,k,l}) \right] \\
%% %%  U_\mathrm{imp}   &= \sum\limits_{i,j,k,l} \frac{1}{2} k_{i,j,k,l} ( \xi_{i,j,k,l} 
%% %%                         - \xi^\mathrm{ref}_{i,j,k,l})^2,
%% %% \end{align}
%% %% where the $k$'s are the force constants, $r$ the minimum-image distance, 
%% %% $\theta$ the angle, $\phi$ the dihedral angle, $m$ the multiplicity, 
%% %% $\xi$ the improper dihedral angle and $i$, $j$, $k$ and $l$ 
%% %% the indices of the involved atoms.
%% %% %
%% %% The $\mathrm{ref}$ superscript denotes the reference length or angle.
%% %% %
%% %% Note that bond lengths are often constrained using the SHAKE\cite{RY77.1} 
%% %% or other algorithms to allow longer timesteps.

Interactions between atoms separated by more than two bonds (including pairs
of atoms in different molecules) are described by non-bonded interaction terms.
These terms depend on the distance between two atoms.
%
There are typically two different terms considered which describe the
electrostatic and van-der-Waals interactions between the atoms.
%
The electrostatic interactions are defined by a Coulomb potential 
%terms
having a  $1/r$ dependence, where $r$ is the distance of the atoms.
These terms model the attraction of two atoms with partial charges of opposite signs,
or the repulsion of atoms with partial charges of the same sign.
%
The van-der-Waals interactions are commonly modeled with a Lennard-Jones functional form,
including the repulsion due to the Pauli exclusion principle for the
electrons at short distances ($r^{-12}$ dependence), and the attraction
due to London dispersion forces ($r^{-6}$ dependence) for longer distances.
%% %% The non-bonded interactions include the Coulomb interactions $U_\mathrm{C}$
%% %% between (partial) charges and the Lennard-Jones interactions $U_\mathrm{LJ}$ 
%% %% mimicking the attraction due to London dispersion forces
%% %% \cite{EI30.1,LO30.1,LO37.1} and % arising from atomic  dipoles 
%% %% %induced by the instantaneous polarization of the electrons and mimicking 
%% %% the repulsion due to the Pauli exclusion principle for the electrons.
%% %% %
Because non-bonded interactions, especially the electrostatic interactions,
decay very slowly, special care has to be taken for long-range non-bonded 
interactions. Otherwise one would need to simulate very large systems
which is computationally expensive.
%
The most prominent methods in this regard are the
reaction field\cite{BE86.3} (RF) and the particle-mesh Ewald\cite{DA93.1} (PME) methods.
%
In the RF method, a cutoff is applied which sets the interaction 
to zero at distances larger than the cutoff distance. 
%By only cutting off the
%electrostatic interactions, one would introduce discontinuities in the potential
%energy and therefore violate the conservation of total energy.
%
To avoid the neglect of longer-range interactions, a reaction-field term 
is added, accounting for the effect of a
% to correct for
%the missing 
dielectric environment beyond the cutoff sphere.
%
\rem{The PME ...XXX.}
%
Since the covalent terms take care of interactions between atoms
separated by up to three bonds, the nonbonded interactions for
first and second neighbors are usually excluded.
Third-neighbor covalent interactions are usually scaled, or
handled using special (reduced) non-bonded interaction parameters.
% are used for third neighbor non-bonded interactions.
%% %% In order to save computation time and avoid spurious effects due to 
%% %% self-interactions across periodic copies, the reaction-field method\cite{BE86.3}
%% %% is used to truncate these interactions after a certain cut-off radius $r_c$
%% %% and add for the Coulomb interactions a correction term to make up 
%% %% for the missing interaction with the dielectric environment.
%% %% %
%% %% The non-bonded interactions are then given by
%% %% %
%% %% \begin{align}
%% %%  U_\mathrm{C}  &= \sum_{i,j} \frac{q_i q_j}{4 \pi \epsilon_0} \left[ \frac{1}{r_{ij}} 
%% %%                   - \frac{(1 - \epsilon_\mathrm{RF})^2 r_{ij}^2 + 3 \epsilon_\mathrm{RF} r_c^2}
%% %%                         {r_c^3 (2 \epsilon_\mathrm{RF} + 1)} \right] \Theta(r_c - r_{ij}) \\
%% %%  U_\mathrm{LJ} &= \sum_{i,j} \left[ \frac{C_{12\,i,j}}{r_{ij}^{12}} 
%% %%                                   - \frac{C_{6\,i,j}}{r_{ij}^{6}} \right] 
%% %%                      \Theta(r_c - r_{ij})  \label{eq:lj_ia}
%% %% \end{align}
%% %% with the Heaviside function
%% %% \begin{align}
%% %%  \Theta(x) = \begin{cases}
%% %%                 0   & x < 0 \\
%% %%                 1   & x \ge 0,
%% %% \end{cases}
%% %% \end{align}
%% %% %
%% %% the distance $r_{ij} = |\fvector{r}_{ij}| = |\fvector{r}_{i} - \fvector{r}_{j}|$,
%% %% the charges $q$, the reaction-field permittivity $\epsilon_\mathrm{RF}$ and
%% %% the Lennard-Jones parameters $C_{12}$ as well as $C_6$.
%% %% %

All force-field terms extensively make use of parameters like force constants,
reference bond lengths, angles, torsions and improper dihedrals,
partial charges and Lennard-Jones parameters. The number of parameters
is on the order of number of degrees of freedom of the molecules considered.
For example, a water (\ce{H2O}) molecule is in principle
described by 10 parameters, including 4 Lennard-Jones parameters
(2 for the oxygen, 2 for the hydrogen, the latter sometimes set to zero),
2 partial charges, and two reference values and two force constants for the
bond lengths and angle, respectively.
%
These parameters have to
be fitted 
%to the molecular system 
in order to reproduce experimental
properties in an MD simulation.
%
Initial guesses of parameters like force constants (energetic information) are
taken from experimental spectroscopic measurements.
%
For reference structural information (bond lengths, angles, torsions and improper dihedrals),
data from X-ray crystallography can be used.
%
All experimentally derivable parameters can be refined or even completely replaced
by quantum-mechanical (QM) estimates.
%calculations.
%
However, the initial parameters taken from experiments or calculations have
to be refined to make them compatible to each other and to the employed
algorithms.
%
Therefore, force-field parameterization is a time consuming and difficult task,
which is still the subject of ongoing research.\rem{[REF]}

\subsection{Integration of the Equations of Motion}
\labsec{eqmot}
%
%After providing 
Given a force field for the interactions of the particles
and after defining the Hamiltonian $\mathcal{H}$ (\refeq{ham}) of the system,
the classical equations of motion in \refeqs{ri} and \refeqn{pi} can be integrated.
%
In view of the large size of the system ($6N$ dimensions), the analytical
integration of the equations of motion is impossible and has to be performed
numerically instead.
%
One employs a finite time step $\Delta t$, which has to be 
short enough to avoid 
quadrature errors and long enough to provide computational
efficiency and to avoid round-off errors. It is typically chosen on the
order of $1/10$ of the fastest timescale of the system, determined by the
bond vibrations, on a femtosecond timescale.
%
Different algorithms aiming at reducing the integration error have been developed.
One example is the leap-frog algorithm\cite{HO70.1} which evolves the positions $\fvector{r}_i$ and 
velocities $\fvector{v}_i$ shifted by a half timestep
%
\begin{align}
 \fvector{v}_i(t + \frac{\Delta t}{2}) &= \fvector{v}_i(t - \frac{\Delta t}{2}) 
                                        + \frac{\fvector{f}_i (\fvector{r}(t))}{m_i} \Delta t \\
 \fvector{r}_i(t + \Delta t) &= \fvector{r}_i(t) + \fvector{v}_i (t + \frac{\Delta t}{2}) \Delta t .
\end{align}
%
This leads to a vanishing $(\Delta t)^2$ term and, therefore, to a reduced error 
on the order of $(\Delta t)^3$.



\subsection{Thermostatting and Barostatting}
%
The integration of the equations of motion
%Hamiltonian 
leads to a microcanonical ensemble (\refsec{cmech}).
%
To reproduce experimental conditions, it is necessary to simulate 
at constant temperature and, possibly, pressure. 
%
The temperature and pressure is regulated by employing thermostats and barostats.

The instantaneous temperature, \ie{} the temperature of a specific state $\xv$
of a system, depends solely on the kinetic energy
% of the system 
as defined by 
the momenta of the particles,
%
\begin{align}
 \mathcal{T} &= \frac{2 \mathcal{K}}{\mathcal{N}_\mathrm{dof} k_\mathrm{B}},\\
 &=\sum_i\frac{\pv_i^2}{m_i\mathcal{N}_\mathrm{dof} k_\mathrm{B}} \nonumber
\end{align}
%
where $N_{\mathrm{dof}}=3N-6$ is the number of degrees of freedom of the system with $N$ particles.
%
In principle one could just constrain the temperature to the desired target temperature
by scaling the momenta of the particles at every integration step, but this leads
to a constrained 
%delta 
temperature, which is
not correct for a canonical or Gibbs ensemble. 
%distribution $\delta(\mathcal{T}-T_{\mathrm{ref}})$
%
Instead, the temperature distribution should be a Boltzmann distribution,
where the instantaneous temperature is allowed to fluctuate.
%% \cite{HU05.1}
This temperature fluctuation is made possible in the weak-coupling thermostat
by Berendsen\cite{BE84.1}, which rescales the velocities  with the factor
%
\begin{align}
 \gamma = \left[ 1 + \frac{\Delta t}{\tau_\mathrm{T}} \left( \frac{T}{\mathcal{T}} - 1 \right) \right]^{1/2},
\end{align}
%
\phicom{gamma does not rescale the temp, but the vel}
%
by employing a coupling time parameter $\tau_{\mathrm{T}}$. The
temperature distribution is still not a Boltzmann distribution, which requires
a more advanced velocity-scaling thermostat like the Nos\'e-Hoover\cite{NO84.1,NO84.2,HO85.1}
or Nos\'e-Hoover chain\cite{MA92.1} thermostats.
% have to employed. 
%
Another means of temperature control 
relies on
%are 
stochastic approaches like the Andersen thermostat,\cite{AN80.1} where the velocities
of the particles are reset to a Maxwell-Boltzmann distribution at constant time periods.
Other equations of motion such as the Langevin equation of motion 
are also able to generate constant-temperature ensembles. Here, the forces are attenuated
by a friction force while additional stochastic forces (``random kicks'') are applied
to the particles.
Pure Monte Carlo approaches with the Metropolis-Hastings acceptance criterion\cite{HA70.4} also automatically generate 
an $NVT$ ensemble. 


The instantaneous pressure $\mathcal{P}$, on the other hand, depends
on both the momenta and positions,
%
\begin{align}
 \mathcal{P} = \frac{1}{3V}\sum_i \frac{\pv_i^2}{m_i}+\rv_i\cdot f_i
,
\end{align}
%
where the product of position and force, $\rv_i\cdot\fvector{f}_i$,  is the virial, 
which accounts for the deviation from ideal-gas behavior.
%ity of the system.
%
To simulate at the correct average pressure, the positions of the particles 
and including the box size are usually rescaled, \eg{},  by the Berendsen 
weak-coupling barostat.\cite{BE84.1}



%% %% \subsection{Ergodic Hypothesis and Boltzmann Probability}
%% %% \label{sec:ergodic_hypothesis}

%% %% The expectation value of any observable $A$ is calculated as the sum of the 
%% %% observable's values $A_i$ in each state 
%% %% multiplied by the corresponding probability $w_i$
%% %% \begin{align}
%% %%  \begin{split}
%% %%     A &= \sum_i A_i w_i \\
%% %%       &= \frac{\sum_i A_i \ee^{-\beta E_i}}{Z} \\
%% %%       &\equiv \langle A_i \rangle,
%% %% \label{eq:observable}
%% %%  \end{split}
%% %% \end{align}
%% %% %
%% %% where $\langle \cdots \rangle$ denotes an ensemble average.
%% %% %
%% %% However, in MD, the ensembles averages are not calculated directly
%% %% but instead the system is dynamically evolved for a certain time.
%% %% %
%% %% According to the ergodic hypothesis\cite{AL87.1}, a system which is simulated 
%% %% for a sufficiently long time $t \rightarrow \infty$, 
%% %% will visit all microstates $i$ in the phase space with a probability 
%% %% proportional to their Boltzmann weight $\ee^{-\beta E_i}$.
%% %% %
%% %% Therefore, the ensemble average of an observable $A$ 
%% %% will be the same as its time average from a trajectory
%% %% \begin{align} 
%% %%  \langle A_i \rangle = \lim_{t \rightarrow \infty} \frac{1}{t} 
%% %%                                 \int_0^t\!A(\tau)\,\dd \tau.
%% %% \end{align} 
%% %% %
%% %% %Thus, $w_i$ can be seen as the probability with which a system 
%% %% %is in a certain state $i$ during a trajectory.







%% %% \subsection{Biasing and Reweighting}


%% %% Even though an infinitely long simulation should in principle visit all 
%% %% relevant microstates, this might not be the case for many simulated systems
%% %% due to the finite simulation time and too large energy barriers.
%% %% %
%% %% A common trick then is to introduce a biasing \cite{TO77.1} potential function 
%% %% $\mathcal{B}$, helping the system to cross barriers and sample all relevant microstates.
%% %% %
%% %% Of course, the ensemble averages of any observable in this so-called biased 
%% %% ensemble will differ from the ensemble averages of the original, unbiased 
%% %% ensemble.
%% %% %
%% %% However, the value of the observable for an unbiased ensemble can be 
%% %% recovered from the biased ensemble \textit{via} reweighting\cite{HA10.1}
%% %% by expanding the fraction of Eq.~\eqref{eq:observable}
%% %% %
%% %% \begin{align}
%% %%  \begin{split}
%% %%      A &= \frac{\sum_i A_i \ee^{+\beta \mathcal{B}_i} \ee^{-\beta (E_i + \mathcal{B}_i)} / Z_b}
%% %%           {\sum_i \ee^{+\beta \mathcal{B}_i} \ee^{-\beta (E_i + \mathcal{B}_i)} / Z_b} \\
%% %%        &= \frac{\sum A_i \ee^{+\beta \mathcal{B}_i} \tilde{w}_i}{\sum \ee^{+\beta \mathcal{B}_i} \tilde{w}_i} \\
%% %%        &= \frac{\langle A_i \ee^{+\beta \mathcal{B}_i} \rangle_b }{\langle \ee^{+\beta \mathcal{B}_i} \rangle_b },
%% %% \label{eq:biased}
%% %%  \end{split}
%% %% \end{align}
%% %% where $\langle \cdots \rangle_b$ is the biased ensemble average,
%% %% $Z_b = \sum_i \tilde{w}_i$ the partition function of the biased ensemble and 
%% %% $\tilde{w}_i = \ee^{-\beta (E_i + \mathcal{B}_i)} / Z_b$ 
%% %% the weights of the microstates in the biased ensemble.
%% %% %
%% %% %In some sense, the additional weights $\ee^{+\beta \mathcal{B}}$ could be seen 
%% %% %as the characteristic function $\Xi$ in the case of free-energy calculations.


\subsection{Free-Energy Calculations}

Many thermodynamic properties like the temperature, pressure, or the density
can be extracted from a simulation by calculating the average over the
corresponding instantaneous variable.%, \ie{} by approximating \refeq{prop} as a sum.
\phicom{equation number clash}
%
For the calculation of free energies, however, it is not a viable alternative.\cite{KI93.1,TU10.1}
%
Although we can express \refeq{partfct} for the free energy as an ensemble average
by noting that $Q(N,V,T)=\langle \exp (\beta \ham (\xv)) \rangle$,
%
\begin{align}
    A(N,V,T)&=-\beta^{-1} \ln \langle \exp (\beta \ham (\xv)) \rangle
\end{align}
%
it is not 
%advisable 
possible to calculate free energies in this way. The ensemble
average will not converge, because configurations $\xv$ with high energies $\ham$
are seldom visited in a simulation (low Boltzmann weight), but have the strongest
impact on the ensemble
average. Thus, 
it would be impossible to obtain
%one would require extremly long (if not infinite) simulations
%to get a 
a converged result.
%
% FEP
But we can take advantage of the fact that one is usually interested
in free-energy differences between two states A and B instead of the absolute free energy, \ie{}
%
\begin{align}
    \Delta A(N,V,T)&= A_\mathrm{B}-A_\mathrm{A} \\
    &= -\beta^{-1} \ln \frac{Q_{\mathrm{B}}}{Q_{\mathrm{A}}}%\\
%    &= -\kb T \ln   \frac{\int_{\mathrm{B}} \mathrm{d}\fvector{p} \int_{\mathrm{B}} \mathrm{d}\fvector{r} \exp(-\beta\mathcal{H}_\mathrm{B}(\pv,\rv))}{\int_{\mathrm{A}} \mathrm{d}\fvector{p} \int_{\mathrm{A}} \mathrm{d}\fvector{r} \exp(-\beta\mathcal{H}_{\mathrm{A}}(\pv,\rv))},
\end{align}
%
where the subscript A or B denote a property pertaining to state A or B, respectively.
%


One distinguishes between thermodynamic, conformational and alchemical free-energy differences, depending on the end states considered.
%
Thermodynamic free-energy differences consider end states which differ in a thermodynamic property like the temperature, pressure or number of molecules.
%
Conformational free-energy differences consider end states which are different parts
of phase space. In other words, the free-energy change upon changing conformations is observed 
along one (or more) degree(s) of freedom of the system.
%
Alchemical free-energy differences consider end states which differ in the Hamiltonian. An alchemical coordinate is added to the system, along which the Hamiltonian is changed, while the degrees of freedom (number of atoms) remain unchanged. Whereas the former free-energy differences are physical, the alchemical one is unphysical, and has no experimental counterpart.
%
But in MD simulations, it often offers a sampling advantage to conduct alchemical calculations and compare the results of such simulations with experimental data \via{} a thermodynamic cycle.
%
An example is the free-energy of binding of a drug molecule to a receptor, which is depicted in \reffig{cycle}. One can alchemically mutate the drug molecule to a dummy skeleton (non-interacting particles), once bound to the receptor and once unbound. 
%
The comparison of these 
%relative difference between those 
two differences  gives access to the  binding free-energy 
%difference of binding 
at considerably lower computational cost than directly calculating the conformational free-energy difference.


\begin{figure}[!htb]
\caption{\footnotesize\captitital{Schematic illustration of a thermodynamic cycle considering the binding of a drug molecule A (magenta) to a receptor (brown).}
The upper horizontal processe defines the conformational change from the unbound receptor and drug molecule to the bound complex, which means the drug molecule is displaced from the bulk water into the binding pocket. The corresponding free-energy difference is $\Delta A_{A}^{\mathrm{bound-unbound}}$.
The lower horizontal processe defines the same conformational change for the dummy state D (non-interacting skeleton). The corresponding free-energy difference is $\Delta A_{D}^{\mathrm{bound-unbound}}$ is zero, because the dummy D is not interacting.
The vertical processes correspond to the alchemical transformation of molecule A to the dummy state D, once in the unbound state and once in the bound state. The total free-energy change around the cycle is zero. Therefore the free-energy difference of interest ($\Delta A_{A}^{\mathrm{bound-unbound}}$) can be inferred from the two alchemical calculations, which can be performed at considerably lower computational cost than the direct conformational calculation.
}
\label{fig:cycle}
\end{figure}

%
For conformational free-energy calculations, the Hamiltonian remains unchanged. Therefore, the ratio between the partition 
%sums 
functions
can be expressed as ratio of probabilities $P_{\mathrm{A}}$ and $P_{\mathrm{B}}$ of being in state A
or B,
\begin{align}
    \Delta A(N,V,T)&=  -\beta^{-1} \ln   \frac{\int_{\mathrm{B}} \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}(\xv))}{\int_{\mathrm{A}} \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}(\xv))} \nonumber \\
%          &= -\beta^{-1} \ln   \frac{\int_{\mathrm{B}} \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}(\xv))}{\int_{\mathrm{A}} \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}(\xv))} \\
                &= -\beta^{-1} \ln   \frac{P_{\mathrm{B}}}{P_{\mathrm{A}}}.
\end{align}
%
\phicom{multiple eq-numbers in eq, line 1 and 2 are identical}
%
Note that the integral is not running over the whole phase space anymore, but is restricted to the different regions 
A and B of the phase space. The probabilities of being in state A and state B are easily
calculated from a simulation and can then be used to determine
the free-energy difference. This approach is frequently used
and termed direct counting. However, it requires sufficient
sampling and interconversion between the two states A and B,
which is only possible if the free-energy difference is small
enough and the states are not separated by a too high barrier.

In alchemical calculations, when $\mathcal{H}_{\mathrm{A}}$ is not 
equal  to $\mathcal{H}_{\mathrm{B}}$, one applies the following 
transformation
%
\begin{equation}
\begin{split}
    \Delta A(N,V,T)& \\        
    = -\beta^{-1} &\ln   \frac{\int \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}_\mathrm{B}(\xv))}{\int \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}_{\mathrm{A}}(\xv)))}  \\
=  -\beta^{-1} &\ln   \frac{\int \mathrm{d}\fvector{x}  \exp(-\beta(\mathcal{H}_{\mathrm{B}}(\xv)-\mathcal{H}_{\mathrm{A}}(\xv))) \exp(-\beta\mathcal{H}_{\mathrm{A}}(\xv))}{\int \mathrm{d}\fvector{x} \exp(-\beta\mathcal{H}_{\mathrm{A}}(\xv))}  \\
=  -\beta^{-1} &\ln   \int \mathrm{d}\fvector{x}  P_{\mathrm{A}}\exp(-\beta(\mathcal{H}_\mathrm{B}(\xv)-\mathcal{H}_{\mathrm{A}}(\xv)))   \\
=  -\beta^{-1} &\ln \langle \exp [-\beta(\mathcal{H}_{\mathrm{B}}(\xv)-\mathcal{H}_{\mathrm{A}}(\xv))] \rangle_{\mathrm{A}} .
\label{eq:zwanzig}
\end{split}
\end{equation}
%
This approach is called free-energy perturbation and \refeq{zwanzig} is the Zwanzig formula\cite{ZW54.1}. In practice one simulates state A (with Hamiltonian $\ham_{\mathrm{A}}$) and one 
%evaluates 
infers the energy of state B
from the configurations sampled at state A. This approach will only yield good results if there is enough phase-space overlap between states A and B, \ie{} if the configurations sampled in 
A are also low-energy conformations for B.

If the phase-space overlap is not 
%given, 
sufficient, 
%one can set up 
a hybrid Hamiltonian can be constructed with a coupling parameter $\lam$ that defines a continuous transformation between the Hamiltonians of the physical end-states $A$ and $B$. The hybrid Hamiltonian $\ham(\xv;\lam)$ must satisfy the boundary conditions 
$\ham(\xv;0)=\ham_A(\xv)$ and $\ham(\xv;1)=\ham_B(\xv)$. This enables splitting up the alchemical transformation into 
several parts by simulating at intermediate states along the coupling parameter and calculating the free-energy differences for only a part of the whole transformation. The 
phase-space overlap between two neighboring states is increased, which leads to better convergence. \phicom{last sentence is ugly}
Finally the total free-energy difference can be obtained as a sum over the parts.

A different and frequently used approach is the thermodynamic integration method\cite{KI33.1,KI34.2,KI35.1}. Here one integrates 
%over 
the
mean
force along the alchemical coupling parameter,
%
\begin{align}
    \Delta A(N,V,T)&= \int\limits_0^1 \frac{\partial A(\lam)}{\partial \lam}\mathrm{d} \lam\ . \label{eq:ti}
\end{align}
%
The derivative of the free energy with respect to the coupling parameter is given by 
%
\begin{align}
    \frac{\partial A(\lam)}{\partial \lam} &= \frac{\partial}{\partial \lam} \left ( -\beta^{-1} \ln \frac{1}{h^{3N}N!} \int \exp (\beta \ham (\xv; \lam)) \dd \xv \right ) \nonumber\\
&= -\beta^{-1} \frac {\frac{\partial}{\partial \lam}  \int \exp (\beta \ham (\xv; \lam)) \dd \xv }{ \int \exp (\beta \ham (\xv; \lam)) \dd \xv} \nonumber \\
&= -\beta^{-1} \frac {  \int \frac{\partial \ham(\xv; \lam)}{\partial \lam} \exp (\beta \ham (\xv; \lam)) \dd \xv }{ \int \exp (\beta \ham (\xv; \lam)) \dd \xv} \nonumber\\
&= \left \langle \frac{\partial \ham}{\partial \lam} \right \rangle _{\lam}, \label{eq:der}
\end{align}
%
\ie{} it is exactly the ensemble average of the Hamiltonian derivative with respect to $\lam$.
%
The free-energy difference is thus calculated by simulating at different $\lam$-values,
calculating the ensemble average of the Hamiltonian derivative at the respective $\lam$ values 
and finally integrating numerically to obtain the final free-energy difference.

\section{Aim of this thesis}

\phicom{watch out for broken refs below}
This thesis deals with methodological developments of free-energy calculations
and their application in MD simulations.
%
In Chapter~\ref{ch:resa}, a force-field for resorcin[4]arenes is presented,
which is employed for the calculation of the free-energy difference between two distinct conformations, a closed
\vase{} conformation with a cavity and an open \kite{} conformation with an expanded surface. To efficiently calculate the free-energy difference, a
method called ball-and-stick local elevation umbrella sampling (B\&S-LEUS) is used.
%
Chapters~\ref{ch:cbti}-\ref{ch:cbus} deal with the development and 
application of the so-called conveyor belt scheme. This scheme employs multiple
coupled replicas which concertedly move on a forward-turn-backward path, akin
a conveyor belt, along a coordinate of interest.
%
In Chapters~\ref{ch:cbti} and \ref{ch:ortho}, the coordinate of interest is the alchemical coupling parameter
 $\lam$, and the scheme is therefore termed conveyor belt thermodynamic integration (CBTI).
CBTI is introduced and applied to the aqueous annihilation of methanol. In Chapter~\ref{ch:ortho}, the performance is compared and
tested on two other systems, namely to the alchemical mutations of parts of a tripeptide and a guanosine triphosphate.
Finally, in Chapter~\ref{ch:cbus}, the conveyor belt scheme is extended to 
conformational changes, now termed conveyor belt umbrella sampling (CBUS),
and applied to the calculation of binding free energies between ions and crown ethers
in various solvents.

